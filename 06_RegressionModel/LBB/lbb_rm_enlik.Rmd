---
title: "LBB Regression Models"
author: "Enlik"
date: "14 February 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# What is Regression Analysis?
In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables.
[Wikipedia](https://en.wikipedia.org/wiki/Regression_analysis).

One of the functions from regression analysis is to build prediction models.

# Introduction
This dataset (insurance) source was from [Kaggle](https://www.kaggle.com/mirichoi0218/insurance). It contains 1,338 observations and 7 variables:

1. age
2. sex
3. bmi (Body Mass Index)
4. children (Their Total Children)
5. smoker
6. region
7. charges (Medical Charges)


# Required Libraries
```{r}
library(MASS)
library(car)
```

# Read data
```{r}
insurance <- read.csv("insurance.csv")

head(insurance)
summary(insurance)
table(insurance$children)

hist(insurance$charges)
boxplot(insurance$charges)
```

From summary above, some simple observations that we've found are:

1. Age range: 18-64 years old
2. Male and female proportion almost balance 50:50
3. BMI range: 15.96 to 53.13
4. Most of them don't have children
5. Smoker only around 25% of total


# Are medical charges have any relationship with other variables?
H0 = no relation between any predictors and the response (tested by F-statistic value). The p-value of F-statistic can be used to determine whether the H0 can be rejected or not.

Here is multiple linear regression model using all the predictors:
```{r}
lm1 <- lm(formula = charges ~ ., data = insurance)
summary(lm1)
```
A high value of F statistic (500.8), with a very low p-value (<2.2e-16) means there is a potential relationship between the predictors and the outcome.

R-squared ($R^2$) measures the proportion of variability in the outcome that can be explained by the model, and is always between 0 and 1. The higher the value, the better the model is able to explain the variability in the outcome.


# Validation of Regression Analysis
## Which variables have a strong relation to "medical charges" variable?

We will finding out if all or only some of the predictors are related to the outcome.

Linear regression using the features with significant p-values only.
```{r}
lm1.sel <- lm(charges~age+bmi+children+smoker+region, data = insurance)
```

Using `stepAIC()`, we will compare this to mixed selection, which is a combination of "forward"" and "backward" selection, to select the best model out of multiple models.
```{r}
step1 <- stepAIC(lm1, direction = "both", trace = FALSE)
```

## Compare Two Models
```{r}
step1$call

lm(formula = charges ~ age + bmi + children + smoker + region, 
    data = insurance)
```

```{r}
lm1.sel$call

lm(formula = charges ~ age + bmi + children + smoker + region, 
    data = insurance)
```
The model given by *stepwise* selection is same as the model we got by selecting the predictors with significant p-values.


# Multicollinearity Assumption Checking
Multicollinearity is a phenomenon when two or more predictors are highly related to each other, and then one predictor can be used to predict the value of the other predictor.

Multicollinearity can be detected using the **Variance Inflation Factor (VIF)**.

Usually, a VIF value of above 5 or 10 is taken as an indicator of multicollinearity.

The simplest way of getting rid of multicollinearity in that case is to discard the predictor with high value of VIF.
```{r}
vif(step1)
```
None of the predictors in this case has a high value of VIF. Because of that, we donâ€™t need to worry about multicollinearity in our case.


# Residual Plot
Residual for any observation is the difference between the actual outcome and the fitted outcome as per the model.
```{r}
residualPlot(step1, type = "rstandard")
summary(step1$residuals)
```
The blue line means there is a smooth pattern between the fitted values and the standard residuals. The curve means slight non-linearity in this data.


