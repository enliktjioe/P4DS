---
title: "ML Capstone - SMS Spam"
author: "Enlik Tjioe"
date: "Updated: March 29, 2019"
output:
  html_document:
    css: style.css
    highlight: tango
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Library
```{r, message=FALSE, warning=FALSE}
# Data Manipulation
library(dplyr)

# Data Visualization
library(ggplot2)
library(plotly)

# Text Mining and Wordcloud
library(tm)
library(e1071)
library(SnowballC)
library(wordcloud)

# Machine Learning
library(caret)
library(ROCR)
library(keras)

# Functional Programming
library(purrr)
```


# Pre-processing Data
```{r}
sms <- read.csv("datasets/SMS/sms.csv")
glimpse(sms)
```

## Proportion of Ham or Spam Count
```{r}
ggplotly(ggplot(sms, aes(x = STATUS, fill = STATUS)) +
  geom_bar(stat = "count"))
```

## Text Mining Process
```{r, message=FALSE, warning=FALSE}
corpus <- VCorpus(VectorSource(sms$CONTAIN))

# Custom function for transform corpus
transformer <- content_transformer(function(x, pattern){
  gsub(pattern, " ", x)
})

# stopword for Indonesian language
stopwords.id <- readLines("datasets/SMS/stopwords-id.txt") 

# Cleaning Corpus Process
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, transformer, "\\n")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords.id)

corpus[[1]]$content
```


```{r}
sms.dtm <- DocumentTermMatrix(corpus)
freqTerms <- findFreqTerms(sms.dtm, 5)
length(freqTerms)
freqTerms[1:10]
```

## Make a wordcloud
```{r}
wordcloud(corpus,
          min.freq = 5,
          max.words = 100,
          random.order = FALSE,
          colors = brewer.pal(8, "Set2"))
```

# Split train and test dataset
```{r}
data.intrain <- sample(nrow(sms.dtm), nrow(sms.dtm)*0.8)
sms.dtm.train <- sms.dtm[data.intrain, ]
sms.dtm.test <- sms.dtm[-data.intrain, ]

corpus.train <- corpus[data.intrain]
corpus.test <- corpus[-data.intrain]

sms.status.train <- sms[data.intrain, ]$STATUS
sms.status.test <- sms[-data.intrain, ]$STATUS
```

```{r}
prop.table(table(sms.status.train))
```

```{r}
prop.table(table(sms.status.test))
```

```{r}
dtm_train <- sms.dtm.train[, freqTerms]
dim(dtm_train)

dtm_test <- sms.dtm.test[, freqTerms]
dim(dtm_test)
```
```{r}
convert_count <-  function(x) {
  y <- ifelse(x > 0, "spam", "ham")
  y
}
```

```{r}
train <- apply(dtm_train, 2, convert_count)
test <- apply(dtm_test, 2, convert_count)

test[1:10, 500:510]
```

# Naive Bayes Model
## Train data
```{r}
set.seed(151)
modelNB <- naiveBayes(train, sms.status.train)
```

## Make a prediction
```{r}
pred <- predict(modelNB, test)
dim(test)
```

## Create Confusion Matrix
```{r}
conf <- confusionMatrix(pred, sms.status.test)
conf
dim(sms.status.test)
```

## Visualize
```{r}
conf_matrix <- as.data.frame(table(pred, sms.status.test))

ggplot(data = conf_matrix, aes(x = pred, y = sms.status.test)) +
  geom_tile(aes(fill = Freq)) +
  geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
  scale_fill_gradient(low = "yellow",
                      high = "orange",
                      trans = "log")
```



## ROC Curve
```{r}
probs <- predict(classifier, test, type = "raw")

pred <- prediction(probs[, "spam"], sms.status.test)
plot(performance(pred, measure = "tpr", x.measure = "fpr"), colorize = TRUE)
```

```{r}
auc_value <- performance(pred, measure = "auc")
auc_value@y.values[[1]]
```
AUC: Area Under the Curve = 0.98. artinya model sudah cukup baik memprediksi kelas positif dan kelas negatif. Cocok digunakan ketika ada label yang tidak seimbang (unbalance).


# Submission Test
```{r}
sub.sms <- read.csv("datasets/SMS/submissionSMS.csv")
glimpse(submission.sms)
```

## Text Mining on SubmissionSMS Data
```{r, message=FALSE, warning=FALSE}
sub.corpus <- VCorpus(VectorSource(sub.sms$CONTAIN))

# Custom function for transform corpus
transformer <- content_transformer(function(x, pattern){
  gsub(pattern, " ", x)
})

# stopword for Indonesian language
stopwords.id <- readLines("datasets/SMS/stopwords-id.txt") 

# Cleaning Corpus Process
sub.corpus <- tm_map(sub.corpus, content_transformer(tolower))
sub.corpus <- tm_map(sub.corpus, transformer, "\\n")
sub.corpus <- tm_map(sub.corpus, removePunctuation)
sub.corpus <- tm_map(sub.corpus, removeNumbers)
sub.corpus <- tm_map(sub.corpus, stripWhitespace)
sub.corpus <- tm_map(sub.corpus, stemDocument)
sub.corpus <- tm_map(sub.corpus, removeWords, stopwords.id)

sub.corpus[[1]]$content
```


```{r}
sub.dtm <- DocumentTermMatrix(sub.corpus)
sub.freqTerms <- findFreqTerms(sub.dtm, 5)
length(sub.freqTerms)
sub.freqTerms[1:10]
```

```{r}
convert_count <-  function(x) {
  y <- ifelse(x > 0, "spam", "ham")
  y
}
```

```{r}
sub.test <- apply(sub.dtm, 2, convert_count)
dim(sub.test)
```

```{r}
sub.pred <- predict(modelNB, sub.test)
sub.sms$STATUS <- sub.pred

write_csv(sub.sms, "enlik_spam_classification.csv")
```


# Conclusion
Naive Bayes had classified SMS with 92% accuracy


# References
[SMS Ham or Spam?](https://www.kaggle.com/devisangeetha/sms-ham-spam)
