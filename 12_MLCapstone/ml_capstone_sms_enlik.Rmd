---
title: "Machine Learning Capstone - SMS Spam Classifier"
author: "Enlik Tjioe"
date: "Updated: March 29, 2019"
output:
  html_document:
    css: style.css
    highlight: tango
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This SMS dataset is collected from real SMS dataset with a spam/ham label for every messages.
In this capstone project, we are going to build a classification model to predict spam from sms texts.

I will using *Naive Bayes* and *Random Forest* model for this project.

# Load Library
We are using `pacman` library for easier install/load library using `p_load()` function
```{r, message=FALSE, warning=FALSE}
# Easy Install/Load Library
library(pacman)

# Data Manipulation
p_load(dplyr)

# Data Visualization
p_load(ggplot2)
p_load(plotly)

# Text Mining and Wordcloud
p_load(tm)
p_load(e1071)
p_load(SnowballC)
p_load(wordcloud)

# Machine Learning
p_load(caret)
p_load(ROCR)
p_load(partykit)
p_load(ranger)

# Functional Programming
p_load(purrr)
```


# Pre-processing Data
```{r}
sms <- read.csv("datasets/SMS/sms.csv")
glimpse(sms)
```

## Proportion of Ham or Spam Count
Using `ggplotly`, we will visualize proportion of ham and spam count from our sms dataset
```{r}
ggplotly(ggplot(sms, aes(x = STATUS, fill = STATUS)) +
  geom_bar(stat = "count"))
```

## Text Mining Process
Using some *text-mining* package, we will transform our sms text into corpus format and then clean it using `tm_map()` function.
```{r, message=FALSE, warning=FALSE}
corpus <- VCorpus(VectorSource(sms$CONTAIN))

# Custom function for transform corpus
transformer <- content_transformer(function(x, pattern){
  gsub(pattern, " ", x)
})

# stopword for Indonesian language
stopwords.id <- readLines("datasets/SMS/stopwords-id.txt") 

# Cleaning Corpus Process
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, transformer, "\\n")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords.id)

corpus[[1]]$content
```

## Create Document Term Matrix
We will create *document term matrix* using cleaned corpus data above
```{r}
sms.dtm <- DocumentTermMatrix(corpus)
freqTerms <- findFreqTerms(sms.dtm, 5)
length(freqTerms)
freqTerms[1:10]
```

## Make a wordcloud
Explore most frequent word to appear using `wordcloud()`
```{r}
wordcloud(corpus,
          min.freq = 5,
          max.words = 100,
          random.order = FALSE,
          colors = brewer.pal(8, "Set2"))
```
From wordcloud above, we can see some of most frequent words such as: kuota, pulsa, bonus, paket, etc


# Split train and test dataset
We will split our sms data into train and test, and we will use that to train our model using data train and evaluate usin data test.
```{r}
data.intrain <- sample(nrow(sms.dtm), nrow(sms.dtm)*0.8)
sms.dtm.train <- sms.dtm[data.intrain, ]
sms.dtm.test <- sms.dtm[-data.intrain, ]

corpus.train <- corpus[data.intrain]
corpus.test <- corpus[-data.intrain]

sms.status.train <- sms[data.intrain, ]$STATUS
sms.status.test <- sms[-data.intrain, ]$STATUS
```

```{r}
prop.table(table(sms.status.train))
```

```{r}
prop.table(table(sms.status.test))
```

```{r}
dtm_train <- sms.dtm.train[, freqTerms]
dim(dtm_train)

dtm_test <- sms.dtm.test[, freqTerms]
dim(dtm_test)
```

We will create function to classify numeric value into *ham* or *spam* class
```{r}
convert_count <-  function(x) {
  y <- ifelse(x > 0, "spam", "ham")
  y
}
```

Implement our own function `convert_count()` into data train and test
```{r}
dtm_train <- apply(dtm_train, 2, convert_count)
dtm_test <- apply(dtm_test, 2, convert_count)

dtm_test[1:10, 500:510]
```

# Naive Bayes Model
In machine learning, naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem with strong (naive) independence assumptions between the features. 

Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem.

## Train Our Model
Using `naiveBayes` function we will train our Naive Bayes model using `sms.status.train` data
```{r}
set.seed(151)
modelNB <- naiveBayes(dtm_train, sms.status.train)
```

## Make Prediction
Make our prediction based on our model
```{r}
pred <- predict(modelNB, dtm_test)
dim(dtm_test)
```

## Create Confusion Matrix
We will check result of confusion matrix from our Naive Bayes model
```{r}
conf <- confusionMatrix(pred, sms.status.test)
conf
dim(sms.status.test)
```

## Visualize Confusion Matrix
We will visualize our confusion matrix into a graph
```{r}
conf_matrix <- as.data.frame(table(pred, sms.status.test))

ggplot(data = conf_matrix, aes(x = pred, y = sms.status.test)) +
  geom_tile(aes(fill = Freq)) +
  geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
  scale_fill_gradient(low = "yellow",
                      high = "orange",
                      trans = "log")
```



## ROC Curve
ROC is the alternative method to check our model performance, inform us about how much our model *TRUE POSITIVE* and *FALSE POSITIVE* value. We will how good our performance from the curve called AUC.

AUC value range from 0 to 1
```{r}
probs <- predict(modelNB, dtm_test, type = "raw")

pred <- prediction(probs[, "spam"], sms.status.test)
plot(performance(pred, measure = "tpr", x.measure = "fpr"), colorize = TRUE)
```

```{r}
auc_value <- performance(pred, measure = "auc")
auc_value@y.values[[1]]
```
AUC: Area Under the Curve = 0.98. That means this model was good enough to predict our *POSITIVE CLASS* and *NEGATIVE CLASS*. It's suitable when our data has unbalance label.

# Submission Test
We will try to predict *spam or ham* class for our `submissionSMS.csv` data.
```{r}
sub.sms <- read.csv("datasets/SMS/submissionSMS.csv")
glimpse(sub.sms)
```

## Text Mining on SubmissionSMS Data
Same as before, we need to transform our data first using text mining method
```{r, message=FALSE, warning=FALSE}
sub.corpus <- VCorpus(VectorSource(sub.sms$CONTAIN))

# Custom function for transform corpus
transformer <- content_transformer(function(x, pattern){
  gsub(pattern, " ", x)
})

# stopword for Indonesian language
stopwords.id <- readLines("datasets/SMS/stopwords-id.txt") 

# Cleaning Corpus Process
sub.corpus <- tm_map(sub.corpus, content_transformer(tolower))
sub.corpus <- tm_map(sub.corpus, transformer, "\\n")
sub.corpus <- tm_map(sub.corpus, removePunctuation)
sub.corpus <- tm_map(sub.corpus, removeNumbers)
sub.corpus <- tm_map(sub.corpus, stripWhitespace)
sub.corpus <- tm_map(sub.corpus, stemDocument)
sub.corpus <- tm_map(sub.corpus, removeWords, stopwords.id)

sub.corpus[[1]]$content
```


```{r}
sub.dtm <- DocumentTermMatrix(sub.corpus)
sub.freqTerms <- findFreqTerms(sub.dtm, 5)
length(sub.freqTerms)
sub.freqTerms[1:10]
```

```{r}
convert_count <-  function(x) {
  y <- ifelse(x > 0, "spam", "ham")
  y
}
```

```{r}
sub.test <- apply(sub.dtm, 2, convert_count)
dim(sub.test)
```

## Predict Using Our Model
We will using our `modelNB` Naive Bayes model that we've created before, and try to predict class from submission sms dataset.
```{r}
sub.pred <- predict(modelNB, sub.test)
sub.sms$STATUS <- sub.pred

# write_csv(sub.sms, "enlik_spam_classification.csv")
```


# References
[Takes too long to create a random forest model for text data](https://community.rstudio.com/t/takes-too-long-to-create-a-random-forest-model-for-text-data/20747/2)

[Wikipedia Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)